{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1973059a-ec5d-461d-bde8-17e3594badb8",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "## directly wrap into a chatbot-like interface via custom MCP client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f2147-00bd-4f4d-8706-d4df101ecf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom snippet to handling threading with asyncio\n",
    "import threading\n",
    "import asyncio\n",
    "\n",
    "class RunThread(threading.Thread):\n",
    "    def __init__(self, func, args, kwargs):\n",
    "        self.func = func\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.result = None\n",
    "        super().__init__()\n",
    "\n",
    "    def run(self):\n",
    "        self.result = asyncio.run(self.func(*self.args, **self.kwargs))\n",
    "\n",
    "def run_async(func, *args, **kwargs):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        loop = None\n",
    "    if loop and loop.is_running():\n",
    "        thread = RunThread(func, args, kwargs)\n",
    "        thread.start()\n",
    "        thread.join()\n",
    "        return thread.result\n",
    "    else:\n",
    "        return asyncio.run(func(*args, **kwargs))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121c3cc-cdc0-47da-955a-fa4ef01f6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "from fastmcp import Client\n",
    "from fastmcp.client.transports import StreamableHttpTransport\n",
    "from fastmcp.tools import Tool\n",
    "from colorama import Fore\n",
    "async def memory_client(query, user_id):\n",
    "    client = Client(transport=StreamableHttpTransport(\"http://127.0.0.1:4200/mcp\"))  # use /mcp path\n",
    "    async with client:\n",
    "        tools: list[Tool] = await client.list_tools()\n",
    "        for tool in tools:\n",
    "            print(f\"Tool: {tool}\")\n",
    "        \n",
    "        result = await client.call_tool(\n",
    "            \"memory_agent\",\n",
    "            {\n",
    "                \"query\": query ,\n",
    "                \"user_id\": user_id\n",
    "            }\n",
    "        )\n",
    "    output=result.content[0].text # mcp response to text , which a list with TextContent in the list, access the text via attribute \n",
    "    ## example below \n",
    "    ### CallToolResult(content=[TextContent(type='text', text=\"That's quite an interesting introduction, Babe the talking pig! I'm excited to meet you and your feathered friend, Rob the chicken. What kind of adventures do you two like to have on the farm?\", annotations=None, meta=None)], structured_content={'result': \"That's quite an interesting introduction, Babe the talking pig! I'm excited to meet you and your feathered friend, Rob the chicken. What kind of adventures do you two like to have on the farm?\"}, data=\"That's quite an interesting introduction, Babe the talking pig! I'm excited to meet you and your feathered friend, Rob the chicken. What kind of adventures do you two like to have on the farm?\", is_error=False)\n",
    "    \n",
    "    print(Fore.CYAN + \"inside mcp client , the respond from memory enabled agent:\\n\", output, Fore.RESET)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29516c45-a39d-493b-bcc1-f52a672376bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore\n",
    "\n",
    "user_id = \"user_1\"\n",
    "print(\"Warm welcome, I am a personal conversational assistant chatbot, \\n I have a very good memory and will keep track on our conversation.\\n when you are done talking to me. \\n Type 'exit' to end the conversation.\")\n",
    "turns=0\n",
    "while True:\n",
    "    # Get user input\n",
    "    turns +=1\n",
    "    query = input()\n",
    "    print(f\"------------------------------------------- turn {str(turns)} ------------------------------------------\\n\")\n",
    "    print(Fore.LIGHTGREEN_EX + \"User:\", query)\n",
    "    \n",
    "    # Check if user wants to exit\n",
    "    if query.lower() == 'exit':\n",
    "        print(\"Thank you for talking to me, I wish you a nice day. Bye for now ^__^b \")\n",
    "        break\n",
    "    \n",
    "    # Handle the query and print the response    \n",
    "    response =    run_async(memory_client, query, user_id)  # blocks for 5 seconds and returns \"hello user\"\n",
    "\n",
    "    print(\"Assistant:\", response, Fore.RESET , \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c09e3-1068-42a4-9a88-ebd00b05f85a",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "## integrating standalone Agent Memory into langGraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b436bf5-06b7-4cb2-beaf-71cc0905daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you can directly instantiate the tool\n",
    "from langchain_community.tools import HumanInputRun\n",
    "from langchain.agents import AgentType, load_tools\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "\n",
    "def get_human_input() -> str:\n",
    "    \"\"\" Put human as decision maker, human will decide whether to start from scratch or load from previous memory\"\"\"\n",
    "    \n",
    "    print(\"Decide whether to load from previous saved memory or not\")\n",
    "    print(\"\"\"\\n\n",
    "            Yes/No            \n",
    "            Enter ONLY Yes or No and nothing else !\"\"\")\n",
    "    contents = []\n",
    "    while True:\n",
    "        try:            \n",
    "            line = input()\n",
    "            if 'y' in line.lower():\n",
    "                tool=\"LoadingMemory\"                \n",
    "                line=tool\n",
    "                \n",
    "            elif 'no' in line.lower():\n",
    "                tool=\"FreshStart\"                \n",
    "                line=tool\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        except EOFError:\n",
    "            break\n",
    "        if line.lower() == 'exit':\n",
    "            print(\"You've chosen : \", tool , \" exiting now ,thank you!\")            \n",
    "            break\n",
    "        contents.append(line)\n",
    "        \n",
    "    return \"\\n\".join(contents)\n",
    "\n",
    "\n",
    "# You can modify the tool when loading\n",
    "\n",
    "ask_human = HumanInputRun(input_func=get_human_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49f40c-de11-484b-b398-fdacbf1667f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first we define GraphState \n",
    "from typing import Dict, TypedDict\n",
    "from typing import TypedDict, Annotated, List, Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "import operator\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "class State(TypedDict):\n",
    "    # The input string\n",
    "    query: str    \n",
    "    user_id: str\n",
    "    human_choice : str\n",
    "    agent_with_memory_response : str\n",
    "    agent_without_memory_response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232f893-ec02-4698-808c-a0c81ecbbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from colorama  import Fore,Style\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", max_tokens=1024)\n",
    "\n",
    "\n",
    "async def restart_memory_client(query, user_id):\n",
    "    client = Client(transport=StreamableHttpTransport(\"http://127.0.0.1:4200/mcp\"))  # use /mcp path\n",
    "    async with client:\n",
    "        tools: list[Tool] = await client.list_tools()\n",
    "        for tool in tools:\n",
    "            print(f\"Tool: {tool}\")\n",
    "        \n",
    "        result = await client.call_tool(\n",
    "            \"restart_memory_agent\",\n",
    "            {\n",
    "                \"query\": query ,\n",
    "                \"user_id\": user_id\n",
    "            }\n",
    "        )\n",
    "    output=result.content[0].text # mcp response to text , which a list with TextContent in the list, access the text via attribute \n",
    "    ## example below \n",
    "    ### CallToolResult(content=[TextContent(type='text', text=\"That's quite an interesting introduction, Babe the talking pig! I'm excited to meet you and your feathered friend, Rob the chicken. What kind of adventures do you two like to have on the farm?\", annotations=None, meta=None)], structured_content={'result': \"That's quite an interesting introduction, Babe the talking pig! I'm excited to meet you and your feathered friend, Rob the chicken. What kind of adventures do you two like to have on the farm?\"}, data=\"That's quite an interesting introduction, Babe the talking pig! I'm excited to meet you and your feathered friend, Rob the chicken. What kind of adventures do you two like to have on the farm?\", is_error=False)\n",
    "    \n",
    "    print(Fore.CYAN + \"inside mcp client , the respond from memory enabled agent:\\n\", output, Fore.RESET)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the functions needed \n",
    "def human_choice_node(state):\n",
    "    # ensure using original prompt \n",
    "    print(Fore.BLUE+ \"state: \" , state)\n",
    "    print(\"---\"*10)\n",
    "    query=state[\"query\"]\n",
    "    \n",
    "    agent_choice=ask_human.invoke(input=query)\n",
    "    print(Fore.CYAN+ \"choosen_agent : \" + agent_choice + Fore.RESET)\n",
    "    return {\"human_choice\": agent_choice , \"query\":query }\n",
    "\n",
    "def memory_execution_node(state):    \n",
    "    query = state[\"query\"]\n",
    "    user_id= state[\"user_id\"]\n",
    "    print(Fore.CYAN + \"user query: \", query , Fore.RESET)\n",
    "    # choosen agent will execute the task\n",
    "    choosen_agent = state['human_choice']\n",
    "    if choosen_agent=='LoadingMemory':\n",
    "        ## logic to load memory \n",
    "        response = run_async(memory_client, query, user_id)  # blocks for 5 seconds and returns \"hello user\"\n",
    "    elif choosen_agent==\"FreshStart\":\n",
    "        ## clear the memory and start afresh\n",
    "        response = run_async(restart_memory_client, query, user_id)  # blocks for 5 seconds and returns \"hello user\"\n",
    "    else:\n",
    "        response=\"Please make sure you made a choice to load pre-existing memory or not.\"\n",
    "        \n",
    "    output=llm.invoke(query)\n",
    "    no_memory_response = output.content\n",
    "    print(Fore.CYAN+ \"agent_output: \\n\" + response + Fore.RESET)\n",
    "\n",
    "    return {\"agent_with_memory_response\": response , \"agent_without_memory_response\": no_memory_response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965d954-8d3b-42ad-9196-971655a63ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Define the two nodes \n",
    "workflow.add_node(\"start\", human_choice_node)\n",
    "workflow.add_node(\"end\", memory_execution_node)\n",
    "\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"start\")\n",
    "workflow.add_edge(\"start\", \"end\")\n",
    "workflow.add_edge(\"end\", END)\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4213e-dc82-49e7-b375-619b4680d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query=\"Hello there, my name is Sofia and I am an young artists, I am very good in drawing realistic human faces and expressions.\"\n",
    "user_id=\"sofia\"\n",
    "respond=app.invoke({\"query\":my_query, \"user_id\":user_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d6589-e2a6-4b26-9d95-3fd9bb642fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query=\"I tried to apply for jobs as illustrator in many different companies, but I cannot seem to get hired. I am quite sad.\"\n",
    "user_id=\"sofia\"\n",
    "respond=app.invoke({\"query\":my_query, \"user_id\":user_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e6229-8c14-4b52-b87f-95316da026ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query=\"Tell me what do you remember about me?\"\n",
    "user_id=\"sofia\"\n",
    "respond=app.invoke({\"query\":my_query, \"user_id\":user_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2b6db-0e79-4b63-befc-9b9cee344517",
   "metadata": {},
   "outputs": [],
   "source": [
    "respond[\"agent_with_memory_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71160cba-ca27-48fd-be43-4ca49a865d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "respond[\"agent_without_memory_response\"]cd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c498f-eded-42cc-8c0a-5dce81577a2d",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "## integrating standalone Agent Memory into llama-index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0781af-a6a3-422f-b9cd-f3e6ce46a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nvidia-haystack==0.1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e9892-221c-4b21-8011-d43fd8d0faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "from fastmcp import Client\n",
    "from fastmcp.client.transports import StreamableHttpTransport\n",
    "from fastmcp.tools import Tool\n",
    "from colorama import Fore\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def haystack_recall_memory(query, user_id):\n",
    "    client = Client(transport=StreamableHttpTransport(\"http://127.0.0.1:4200/mcp\"))  # use /mcp path\n",
    "    async with client:\n",
    "        tools: list[Tool] = await client.list_tools()\n",
    "        for tool in tools:\n",
    "            print(f\"Tool: {tool}\")        \n",
    "        result = await client.call_tool(\n",
    "            \"fetch_memory_items\",\n",
    "            {\n",
    "                \"query\": query ,\n",
    "                \"user_id\": user_id\n",
    "            }\n",
    "        )\n",
    "        print(Fore.GREEN +\"fetch_memory_items result type \", type(result), result, Fore.RESET)\n",
    "        output=result.content[0].text\n",
    "        output=ast.literal_eval(output)\n",
    "        print(Fore.GREEN +\"fetch_memory_items output type \", type(output), output, Fore.RESET)\n",
    "    return output\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f0378-b5c2-41f0-89e4-e8ec3730a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_integrations.components.generators.nvidia import NvidiaGenerator\n",
    "\n",
    "generator = NvidiaGenerator(\n",
    "    model=\"meta/llama-3.1-405b-instruct\",\n",
    "    api_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    model_arguments={\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.7,\n",
    "        \"max_tokens\": 1024,\n",
    "    },\n",
    ")\n",
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954dcf0-73e7-450b-b8d5-3f17717fa018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ad595-bb0d-41d5-9c63-d58a74afa166",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def wrap_in_haystack(query,user_id):\n",
    "    \n",
    "    recall_memories = await haystack_recall_memory(query, user_id)\n",
    "    print(type(recall_memories), recall_memories)\n",
    "    if isinstance(recall_memories, list):\n",
    "        print(\"recall_memories is already a list of strings\"  )\n",
    "        if len(recall_memories)>0:\n",
    "            #memories = [Document(content=memory) for memory in recall_memories]\n",
    "            memories_str=','.join([mem for mem in recall_memories])\n",
    "    else: \n",
    "        recall_memories = ast.literal_eval(recall_memories)\n",
    "\n",
    "        memories_str =f\"no memories item found for user {user_id}\"\n",
    "    \n",
    "    prompt_template = f\"\"\"\n",
    "    Given these recalled memories, continue the conversation with the user, ask more question and make the user feel you are eager to find out more about him or her.\\nDocuments:\n",
    "    {memories_str}\n",
    "    \\nQuestion: {query}\n",
    "    \\nAnswer:\n",
    "    \"\"\"\n",
    "    output=generator.run(prompt_template)\n",
    "    response=output['replies'][0]\n",
    "    print(\"---\"*10)\n",
    "    print(Fore.LIGHTMAGENTA_EX + \"respond :\\n\", response)\n",
    "    return response, recall_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9c571-2af8-4c3e-9413-ebd2262a3087",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hello, my name is Kevin and I am a software engineer\"\n",
    "user_id=\"kevin\"\n",
    "output=await wrap_in_haystack(query,user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb496ca-e3a1-4499-bdb4-ec76524261db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8f82d-9d62-47bb-9b60-f3ab9238fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"My favorite movie is interstella and forest Gump\"\n",
    "user_id=\"kevin\"\n",
    "output=await wrap_in_haystack(query,user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ba91f-8e28-4d10-8268-99441f5be444",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me what you remember about me?\"\n",
    "user_id=\"kevin\"\n",
    "output=await wrap_in_haystack(query,user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b56ef-1235-4feb-abef-43a133d31f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f2fbe-c999-444b-a9de-48712fb0ee1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
